{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254a3e4e-af45-4fd3-bde5-05f9e70d8f93",
   "metadata": {},
   "source": [
    "# 2019 USC - Long Item Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a13c2-8782-4e33-b3f7-a1af83561794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "import numpy as np\n",
    "import regex\n",
    "import natsort\n",
    "import json\n",
    "from lxml import etree\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "from quantlaw.utils.networkx import load_graph_from_csv_files\n",
    "\n",
    "%run longitem_support.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272088d-e77e-430d-9a0c-a23f01824bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5096250-db1d-45f1-a22a-9b4534724855",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355237a-64ba-417d-ab6a-ba475e138a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = load_graph_from_csv_files(\"../../legal-networks-data/us/4_crossreference_graph/detailed/\", \"2019\")\n",
    "\n",
    "# find out the chapter and title of each node\n",
    "quotient_G, quotient_nodes_mapping = quotient_graph_with_merge(G, self_loops=False, heading_regex=chapter_regex)\n",
    "nx.set_node_attributes(G, quotient_nodes_mapping, 'chapter_mapping')\n",
    "quotient_G, quotient_nodes_mapping = quotient_graph_with_merge(G, self_loops=False, heading_regex=title_regex)\n",
    "nx.set_node_attributes(G, quotient_nodes_mapping, 'title_mapping')\n",
    "\n",
    "# mark all chapters and titles as such\n",
    "attrib_update =  {x: 'chapter' \n",
    "                      for x,y in G.nodes(data=True)\n",
    "                          if 'heading' in y\n",
    "                          and chapter_regex.match(y['heading'])\n",
    "                  }\n",
    "nx.set_node_attributes(G, values = attrib_update, name='type')\n",
    "\n",
    "attrib_update =  {x: 'title'\n",
    "                      for x,y in G.nodes(data=True)\n",
    "                          if 'heading' in y\n",
    "                          and title_regex.match(y['heading'])\n",
    "                 }\n",
    "nx.set_node_attributes(G, values = attrib_update, name='type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daed77e-8cbe-4623-a3b5-faab4e461b21",
   "metadata": {},
   "source": [
    "### Utility Functions for Plotting / Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb08785-523d-4054-a83a-36b0539ce6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_groups(G, group_mapping, item_type, grouped_by, use_heading = True):\n",
    "    \"\"\"\n",
    "    Takes grouped input (i.e., a list of mapping from groups to all items belonging to that group),\n",
    "    and builds up DataFrame containing token statistic for each item of each group\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for group in group_mapping.keys():\n",
    "        for compare_item in group_mapping[group]:\n",
    "            heading_str = normalize_heading(G.nodes[group]['heading'].split('-',1)[0]) if use_heading else group\n",
    "            data.append([compare_item, heading_str, G.nodes[compare_item]['tokens_n']])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns = [item_type, grouped_by, 'Tokens'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bcf2b-3973-42dd-897b-ca93fd38a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_data(grouped_by, group_df, sample_size = None, plot_func=sns.swarmplot, figsize=(15,8)):\n",
    "    \"\"\"\n",
    "    Plots token statistic for all groups as plot_func (e.g., swarmplot or boxplot)\n",
    "    \"\"\"\n",
    "    if sample_size is not None and sample_size < group_df.shape[0]:\n",
    "        group_df = group_df.sample(n=sample_size)\n",
    "        \n",
    "    group_df = group_df.sort_values(grouped_by, key=natsort.natsort_keygen())    \n",
    "        \n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    if plot_func not in [sns.boxplot, sns.boxenplot]:\n",
    "        ax = plot_func(x=grouped_by, y=\"Tokens\", data=group_df, size=2) #boxplot-variants don't support size argument\n",
    "    else:\n",
    "        ax = plot_func(x=grouped_by, y=\"Tokens\", data=group_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071c325-8dc4-4fa6-a1e2-e00e883a7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ranking_df(compare_level, group_df, G):\n",
    "    \"\"\"\n",
    "    Builds a pd.DataFrame that is sorted by Token count, and resolves foreign keys\n",
    "    to information like chapter titles.\n",
    "    \"\"\"\n",
    "    \n",
    "    _df = group_df.copy().sort_values(['Tokens'], ascending=False)\n",
    "    additional_given_columns = list(_df.columns)\n",
    "    \n",
    "    if 'chapter' not in additional_given_columns:\n",
    "        _df['chapter'] = _df.apply(lambda row: G.nodes(data=True)[row[compare_level]]['chapter_mapping'], axis=1)\n",
    "        _df['chapter'] = _df.apply(lambda row: normalize_heading(G.nodes(data=True)[row['chapter']]['heading'].split('-',1)[0]), axis=1)\n",
    "    else:\n",
    "        additional_given_columns.remove('chapter') \n",
    "        \n",
    "    if 'title' not in additional_given_columns:\n",
    "        _df['title'] = _df.apply(lambda row: G.nodes(data=True)[row[compare_level]]['title_mapping'], axis=1)\n",
    "        _df['title'] = _df.apply(lambda row: normalize_heading(G.nodes(data=True)[row['title']]['heading'].split('-',1)[0]), axis=1)\n",
    "    else:\n",
    "        additional_given_columns.remove('title')\n",
    "    \n",
    "    additional_given_columns.remove('Tokens')\n",
    "    if compare_level in additional_given_columns:\n",
    "        additional_given_columns.remove(compare_level)\n",
    "        \n",
    "    _df['law_name'] = _df.apply(lambda row: G.nodes(data=True)[row[compare_level]]['law_name'], axis=1)\n",
    "    _df['heading'] = _df.apply(lambda row: G.nodes(data=True)[row[compare_level]]['heading'], axis=1)\n",
    "        \n",
    "    _df = _df.reindex(columns=[compare_level, 'Tokens', 'title', 'chapter', 'law_name', 'heading'] + additional_given_columns)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91819483-518d-43d3-b0d5-943135065db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_relative_ranking_df(ranking_df, reference=None, limit_within_group=None, global_sort=False):\n",
    "    \"\"\"\n",
    "    Builds up relative ranked data frame (Table 6 of paper), i.e., adds `RelTokens` column to a ranking_df.\n",
    "    If reference is None, we compare the lengths always to the global next longest item,\n",
    "    otherwise, within the same reference.\n",
    "    \"\"\"\n",
    "    _df = ranking_df.copy()\n",
    "\n",
    "    if reference is not None:\n",
    "        _grouped_df = _df.sort_values([reference, 'Tokens']).groupby(reference)\n",
    "        _df[\"RelTokens\"] = _grouped_df[\"Tokens\"].transform(lambda x: x.rolling(2).apply(lambda x: x.iloc[1] / max(x.iloc[0],1)))\n",
    "\n",
    "        if global_sort:\n",
    "            if limit_within_group is not None:\n",
    "                _df = _df.sort_values([reference, 'RelTokens'], ascending=False)\n",
    "                _df = _df.groupby(reference).head(limit_within_group).reset_index(drop=True)\n",
    "\n",
    "            _df = _df.sort_values(['RelTokens'], ascending=False)\n",
    "        else:\n",
    "             _df = _df.sort_values([reference, 'RelTokens'], ascending=False)\n",
    "             if limit_within_group is not None:\n",
    "                 _df = _df.groupby(reference).head(limit_within_group).reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        _df = _df.sort_values(['Tokens'])\n",
    "        _df[\"RelTokens\"] = _df.rolling(2).apply(lambda x: x.iloc[1] / x.iloc[0])\n",
    "        _df = _df.sort_values(['Tokens'], ascending=False)\n",
    "\n",
    "\n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3b20d-3fdb-4852-aa3e-10a0ff2f8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_no_mapping = dict() # global state that maps title no to node of title, we build this lazily\n",
    "\n",
    "def belongs_to_usc_title_filter(G, node, title_no):\n",
    "    \"\"\"\n",
    "    Returns true if node `node` is part of title `title_no`\n",
    "    \"\"\"\n",
    "    if title_no not in title_no_mapping.keys():\n",
    "        candidates =  list(set([x for x,y in G.nodes(data=True) if not x.startswith('cfr') and 'heading' in y and normalize_heading(y['heading']).startswith(f'Title {title_no}-')]))\n",
    "        if len(candidates) != 1:\n",
    "            raise ValueError(f'Did not find title uniquely in G: Possible titles = {candidates}')\n",
    "        \n",
    "        title_no_mapping[title_no] = candidates[0]\n",
    "    \n",
    "    # as soon as we know what node it is, just check whether the mapping points to the correct node (or if we are the title node itself)\n",
    "    return node == title_no_mapping[title_no] or G.nodes(data=True)[node]['title_mapping'] == title_no_mapping[title_no]\n",
    "\n",
    "def usc_only_filter(G, node):\n",
    "    return not G.nodes(data=True)[node]['key'].startswith('cfr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974230af-0019-4397-b0c1-9b6f31c64aa6",
   "metadata": {},
   "source": [
    "## Analysis 1: Distribution of Item Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b062c-c869-49fa-a810-16a252cf83ad",
   "metadata": {},
   "source": [
    "### Example 1: Analyze Seqitems of Title 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4f573-d1c5-49de-8144-1c230abd5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_LEVEL = \"seqitem\"\n",
    "GROUP_BY = \"chapter\"\n",
    "TITLE = 20\n",
    "\n",
    "# we want to compare the chapters of title 17 USC, based on the seqitems of the chapters\n",
    "group_by = group_items_by(G,\n",
    "                          COMPARE_LEVEL,\n",
    "                          GROUP_BY,\n",
    "                          group_by_filter = lambda x: belongs_to_usc_title_filter(G, x, TITLE)\n",
    "                         )\n",
    "\n",
    "group_df = get_data_for_groups(G, group_by, COMPARE_LEVEL, GROUP_BY, use_heading=True)\n",
    "ranking_df = build_ranking_df(COMPARE_LEVEL, group_df, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14eeb7-5eb7-41d1-9d99-78506c1ab0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longest Seqitems of Title 20\n",
    "ranking_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f2214-4011-4d67-a84c-8ea0d783898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Seqitems, grouped by chapter of Title 20\n",
    "plot_group_data(GROUP_BY, group_df, sample_size = 20000, plot_func=sns.stripplot) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa64cb-c8c6-4c52-836e-e2a56983ecf7",
   "metadata": {},
   "source": [
    "### Example 2: Analyze all USC Seqitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700d7e1-8029-4bc5-b80d-91601f0f6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_LEVEL = \"seqitem\"\n",
    "GROUP_BY = \"title\"\n",
    "\n",
    "# we filter out CFR nodes\n",
    "group_by = group_items_by(G,\n",
    "                          COMPARE_LEVEL,\n",
    "                          GROUP_BY,\n",
    "                          group_by_filter = lambda x: usc_only_filter(G, x)\n",
    "                         )\n",
    "\n",
    "group_df = get_data_for_groups(G, group_by, COMPARE_LEVEL, GROUP_BY, use_heading=True)\n",
    "ranking_df = build_ranking_df(COMPARE_LEVEL, group_df, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfcfbf-d4b3-46d7-bf33-86fb0e06736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative longest seqitems (min. 150 tokens to filter out shorts like 20 vs 2 tokens) with reference to the next longest seqitem _within the same Title_\n",
    "relative_df = build_relative_ranking_df(ranking_df, reference=GROUP_BY, global_sort=True)\n",
    "relative_df.loc[relative_df[\"Tokens\"] > 150].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141816e-c1dc-4890-8163-9a58a894c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute longest seqitems, with information on the relative length to the next longest seqitem _over all titles_\n",
    "result_df = build_relative_ranking_df(ranking_df, global_sort=True) # because there is no reference given, this function just sorts globally by token length and adds the RelToken column\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172b5fa-0624-49d5-992e-3199539cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Seqitems, grouped by all titles\n",
    "plot_group_data(GROUP_BY, group_df, sample_size = 20000, plot_func=sns.stripplot) \n",
    "plt.savefig('../writing/figures/seqitems_group_by_title.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bfeb6-e88c-4672-b36c-bda2c03b1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-Log-Plot of USC Seqitems\n",
    "tokens = group_df.loc[(group_df['Tokens'] > 0)]['Tokens'].to_numpy()\n",
    "fig = powerlaw.plot_pdf(tokens)\n",
    "fig.set_xlabel(\"# Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69381f19-c0bf-46e4-a0be-6031869f638f",
   "metadata": {},
   "source": [
    "### Example 3: Analyze all USC Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffbd53-70e9-4d4c-ba8b-9c9cfc8be779",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARE_LEVEL = \"chapter\"\n",
    "GROUP_BY = \"title\"\n",
    "\n",
    "# we filter out CFR nodes\n",
    "group_by = group_items_by(G,\n",
    "                          COMPARE_LEVEL,\n",
    "                          GROUP_BY,\n",
    "                          group_by_filter = lambda x: usc_only_filter(G, x)\n",
    "                         )\n",
    "\n",
    "group_df = get_data_for_groups(G, group_by, COMPARE_LEVEL, GROUP_BY, use_heading=True)\n",
    "ranking_df = build_ranking_df(COMPARE_LEVEL, group_df, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0d814-d90e-46dd-bb39-8df989af4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f1f9a-862a-4763-96de-846bfd8218a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of chapter lengths, grouped by all titles\n",
    "plot_group_data(GROUP_BY, group_df, sample_size = 20000, plot_func=sns.stripplot) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5311e4-531e-446f-9f93-d9877b0c10d2",
   "metadata": {},
   "source": [
    "### Example 4: Icicle Zoom Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e59c4-631d-43a1-bccd-7f69916197bf",
   "metadata": {},
   "source": [
    "### Icicle Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e138f-22a7-4658-b6b6-8bc84cbc8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "def draw_icicle(df, last_page, levelheight, facecolor, figsize=(9,6), savepath=None, extensions=['svg', 'pdf', 'png'], with_toplevel=True, extra_y_gap = 0):\n",
    "    token_scale_factor = 1000 # in our terms, pages are tokens - scale down by 1000\n",
    "    \n",
    "    last_page = last_page / token_scale_factor \n",
    "    \n",
    "    # initialize plot\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "    y_upper = 10*(max(df.level)+1)+max(df.level)*extra_y_gap\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        level = row['level']\n",
    "        width = row['tokens'] / token_scale_factor\n",
    "        page = row['start_token'] / token_scale_factor\n",
    "        \n",
    "        if level >= (0 if with_toplevel else 1):\n",
    "            _x = page if width > 0 else page-0.5\n",
    "            _y = y_upper-levelheight*(level+1 if with_toplevel else level)-level*(0.05+extra_y_gap)\n",
    "            _width = width-0.2 if width > 0 else 0.3\n",
    "            ax.add_patch(Rectangle(xy=(_x,_y), \n",
    "                                   width=_width, height=levelheight, facecolor=(facecolor if row['zoomed'] == 0 else 'red'), \n",
    "                                   edgecolor='k', linewidth=0.1))\n",
    "            \n",
    "            if row['zoomed'] != 0:\n",
    "                ax.plot([_x, 0], [_y, y_upper-levelheight*(level+2 if with_toplevel else level+1)-(level+1)*(0.05+extra_y_gap) + levelheight], color='red', linestyle='-', linewidth=1)\n",
    "                ax.plot([_x + _width, last_page], [_y, y_upper-levelheight*(level+2 if with_toplevel else level+1)-(level+1)*(0.05+extra_y_gap) + levelheight], color='red', linestyle='-', linewidth=1)\n",
    "\n",
    "                plt.xlim(0,last_page+2)\n",
    "                \n",
    "    plt.ylim(0,y_upper+2)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if savepath is not None:\n",
    "        for ext in extensions:\n",
    "            plt.savefig(f'{savepath}.{ext}', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7d7e-f528-484f-a76b-06e523056816",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617e062-6474-42fd-bab1-b93ab823755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after executing this cell, the `ici_data` list object contains all information necessary to plot an zoomed icicle plot\n",
    "chosen_title = \"Title 15\"\n",
    "chosen_chap = \"Chapter 14A\"\n",
    "\n",
    "# Initialize variables\n",
    "ici_data = [] # key, name, level, tokens, start_token\n",
    "titles = [(x, int(normalize_heading(y['heading'].split('-',1)[0]).split(' ')[1])) for x,y in G.nodes(data=True) if 'type' in y and y['type'] == 'title' and not y['key'].startswith('cfr')]\n",
    "sorted_titles = sorted(titles, key=lambda tup: tup[1])\n",
    "count = 0\n",
    "curr_title_token = 1\n",
    "title_idx = 0\n",
    "important_chapter_rows = []\n",
    "\n",
    "for title_key, title_no in sorted_titles:\n",
    "    chapter_rows = []\n",
    "    chapters = [(x, normalize_heading(y['heading'].split('-',1)[0]).split(' ')[1]) for x,y in G.nodes(data=True) if 'type' in y and y['type'] == 'chapter' and y['title_mapping'] == title_key]\n",
    "    sorted_chapters = natsort.natsorted(chapters, key=lambda tup: tup[1])\n",
    "    curr_chapter_token = curr_title_token\n",
    "    \n",
    "    for chapter_key, chapter_no in sorted_chapters:\n",
    "        seqitems = [(x, y['heading'].split(' ')[1]) for x,y in G.nodes(data=True) if 'type' in y and y['type'] == 'seqitem' and y['chapter_mapping'] == chapter_key]\n",
    "        sorted_items = natsort.natsorted(seqitems, key=lambda tup: tup[1])\n",
    "        seqitem_rows = []\n",
    "        \n",
    "        curr_seq_token = curr_chapter_token\n",
    "        for seq, _ in sorted_items:\n",
    "            seqitem_rows.append([seq, G.nodes(data=True)[seq]['heading'], 2, G.nodes(data=True)[seq]['tokens_n'], curr_seq_token, 0])\n",
    "            curr_seq_token = curr_seq_token + G.nodes(data=True)[seq]['tokens_n']\n",
    "        \n",
    "        tokens = np.array([row[3] for row in seqitem_rows])\n",
    "        zoomed = 1 if normalize_heading(G.nodes(data=True)[chapter_key]['heading'].split('-')[0]) == chosen_chap else 0\n",
    "        chapter_row = [chapter_key, G.nodes(data=True)[chapter_key]['heading'], 1, np.sum(tokens), curr_chapter_token, zoomed]\n",
    "        chapter_rows.append(chapter_row)\n",
    "        if normalize_heading(G.nodes(data=True)[title_key]['heading'].split('-')[0]) == chosen_title:\n",
    "            important_chapter_rows.append(chapter_row)\n",
    "            \n",
    "        chapter_rows = chapter_rows + seqitem_rows\n",
    "        \n",
    "        if normalize_heading(G.nodes(data=True)[chapter_key]['heading'].split('-')[0]) == chosen_chap and normalize_heading(G.nodes(data=True)[title_key]['heading'].split('-')[0]) == chosen_title:\n",
    "            important_chapter_rows = important_chapter_rows + seqitem_rows\n",
    "                \n",
    "        curr_chapter_token = curr_chapter_token + np.sum(tokens)\n",
    "    \n",
    "    zoomed = 1 if normalize_heading(G.nodes(data=True)[title_key]['heading'].split('-')[0]) == chosen_title else 0\n",
    "    tokens = np.array([row[3] for row in chapter_rows if row[2] == 1])\n",
    "    ici_data.append([title_key, G.nodes(data=True)[title_key]['heading'], 0, np.sum(tokens), curr_title_token, zoomed])\n",
    "        \n",
    "    curr_title_token = curr_title_token + np.sum(tokens)\n",
    "    title_idx = title_idx + 1\n",
    "    \n",
    "ici_data = ici_data + important_chapter_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c30684-3df5-42eb-85fd-f0666080c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to normalize and rescale our data\n",
    "ici_data_df = pd.DataFrame(data=ici_data, columns=['key', 'heading', 'level', 'tokens', 'start_token', 'zoomed'])\n",
    "max_token_row = ici_data_df.loc[ici_data_df['start_token'].idxmax()]\n",
    "level0_tokens = max_token_row['start_token'] + max_token_row['tokens']\n",
    "\n",
    "# first, move level1 to the left\n",
    "level1_starttoken = ici_data_df.loc[ici_data_df.loc[ici_data_df['level'] == 1]['start_token'].idxmin()]['start_token']\n",
    "ici_data_df.loc[ici_data_df['level'] == 1, 'start_token'] = ici_data_df.loc[ici_data_df['level'] == 1, 'start_token'].subtract(level1_starttoken)\n",
    "# now, rescale\n",
    "max_level1_token_row = ici_data_df.loc[ici_data_df.loc[ici_data_df['level'] == 1]['start_token'].idxmax()]\n",
    "level1_tokens = max_level1_token_row['start_token'] + max_level1_token_row['tokens']\n",
    "level1_factor = level0_tokens / level1_tokens\n",
    "ici_data_df.loc[ici_data_df['level'] == 1, 'tokens'] = ici_data_df.loc[ici_data_df['level'] == 1, 'tokens'].mul(level1_factor)\n",
    "ici_data_df.loc[ici_data_df['level'] == 1, 'start_token'] = ici_data_df.loc[ici_data_df['level'] == 1, 'start_token'].mul(level1_factor)\n",
    "\n",
    "# shift level 2\n",
    "level2_starttoken = ici_data_df.loc[ici_data_df.loc[ici_data_df['level'] == 2]['start_token'].idxmin()]['start_token']\n",
    "ici_data_df.loc[ici_data_df['level'] == 2, 'start_token'] = ici_data_df.loc[ici_data_df['level'] == 2, 'start_token'].subtract(level2_starttoken)\n",
    "# rescale\n",
    "max_level2_token_row = ici_data_df.loc[ici_data_df.loc[ici_data_df['level'] == 2]['start_token'].idxmax()]\n",
    "level2_tokens = max_level2_token_row['start_token'] + max_level2_token_row['tokens']\n",
    "level2_factor = level0_tokens / level2_tokens\n",
    "ici_data_df.loc[ici_data_df['level'] == 2, 'tokens'] = ici_data_df.loc[ici_data_df['level'] == 2, 'tokens'].mul(level2_factor)\n",
    "ici_data_df.loc[ici_data_df['level'] == 2, 'start_token'] = ici_data_df.loc[ici_data_df['level'] == 2, 'start_token'].mul(level2_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ac341-04aa-4e36-b664-46407a204e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_icicle(ici_data_df,\n",
    "         curr_title_token + 1,\n",
    "         10,\n",
    "         'silver',\n",
    "         figsize=(20,5),\n",
    "         savepath='../writing/figures/longitem_icicles',\n",
    "         extensions=['pdf'],\n",
    "         extra_y_gap=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8011593-336b-4468-9ea1-46c16ea192dc",
   "metadata": {},
   "source": [
    "## Analysis 2: Flagging Long Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04680ab-3729-4988-af01-93aa7a98079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_long_elements(G, element_type, absolute_threshold=10000, relative_threshold=1.5, relative_reference_group=\"title\", relative_min_tokens=50, usc_only=True):\n",
    "    absolute_long_elements = []\n",
    "    relative_long_elements = []\n",
    "    mapping = f\"{relative_reference_group}_mapping\"\n",
    "    \n",
    "    use_relative = relative_threshold is not None and relative_reference_group is not None\n",
    "    \n",
    "    # 0. Build up relative_df to have statistics even for only absolute flags\n",
    "    group_by = group_items_by(G,\n",
    "                              element_type,\n",
    "                              relative_reference_group,\n",
    "                              group_by_filter = lambda x: usc_only_filter(G, x) if usc_only else lambda x: True\n",
    "                             )\n",
    "\n",
    "    group_df = get_data_for_groups(G, group_by, element_type, relative_reference_group, use_heading=True)\n",
    "    ranking_df = build_ranking_df(element_type, group_df, G)\n",
    "    relative_df = build_relative_ranking_df(ranking_df, reference=relative_reference_group, global_sort=True)\n",
    "    \n",
    "    # 1. Find all absolutely long elements\n",
    "    if absolute_threshold is not None:\n",
    "        absolute_long_elements = [x for x,y in G.nodes(data=True) if is_node_of_type(y, element_type) and y[\"tokens_n\"] > absolute_threshold and (not y[\"key\"].startswith('cfr') or not usc_only)]\n",
    "    \n",
    "    # 2. Find all relative long elements\n",
    "    if use_relative:\n",
    "        relative_long_elements = relative_df.loc[(relative_df[\"Tokens\"] > relative_min_tokens) & (relative_df[\"RelTokens\"] > relative_threshold)][element_type].tolist()\n",
    "    \n",
    "    # 3. Build pd.DataFrame (Vertex_ID, Tokens, Absolute_or_Relative_or_Both, ReferenceGroup)\n",
    "    \n",
    "    shared_long_elements = list(set(absolute_long_elements) & set(relative_long_elements))\n",
    "    absolute_long_elements = [e for e in absolute_long_elements if e not in shared_long_elements]\n",
    "    relative_long_elements = [e for e in relative_long_elements if e not in shared_long_elements]\n",
    "    \n",
    "    data = [[e, G.nodes(data=True)[e][\"tokens_n\"], relative_df[relative_df[element_type] == e].iloc[0][\"RelTokens\"],\"Both\", G.nodes(data=True)[e][mapping] if use_relative else \"\"] for e in shared_long_elements]\n",
    "    data += [[e, G.nodes(data=True)[e][\"tokens_n\"], relative_df[relative_df[element_type] == e].iloc[0][\"RelTokens\"],\"Absolute\", G.nodes(data=True)[e][mapping] if use_relative else \"\"] for e in absolute_long_elements]\n",
    "    data += [[e, G.nodes(data=True)[e][\"tokens_n\"], relative_df[relative_df[element_type] == e].iloc[0][\"RelTokens\"],\"Relative\", G.nodes(data=True)[e][mapping] if use_relative else \"\"] for e in relative_long_elements]\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=[\"VertexID\", \"Tokens\", \"RelTokens\", \"Absolute_or_Relative_or_Both\", \"ReferenceGroup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a668de-1603-4683-b63e-040f077958be",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_long_elements(G, \"seqitem\").head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
